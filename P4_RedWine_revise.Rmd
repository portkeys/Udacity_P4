---
title: "Red Wine Quality Prediction"
author: "Wen Yang"
date: "February 20, 2017"
output: 
  html_document: 
    number_sections: yes
    toc: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)

# Import libraries
library(ggplot2)
library(MASS) 
library(dplyr)
library(gridExtra)
library(corrplot)
library(PerformanceAnalytics)
library(reshape2)
library(xtable)
library(grid)
library(caret)
library(randomForest)
library(ggthemes)
library(psych)
library(polycor)
# library(plotly)
```

# Introduction

The objective of this study is to build a classification model to predict red wine quality based on given physicochmeical characteristics. The dataset used for this analysis includes 1599 red wine samples with 11 physicochemical attributes like *alcohol* or *pH*, and 1 sensory attribute called *quality*. More details on the dataset can be referred to the original paper [Cortez et al., 2009](http://projects.csail.mit.edu/wiki/pub/Evodesign/SensoryEvaluationsDatabase/winequality09.pdf).

# Exploratory Data Analysis (EDA)

In EDA phase, data is mainly explored by graphical visualizations assisted with descriptive statistics to help understand the distribution of univariate as well as the the association relationships between variables. EDA discovery will lay the foundation to help select important attributes to build red wine quality prediction model.

To begin with, let's load the Red Wine csv file and preview the dataset.

```{r}
dta <- read.csv('wineQualityReds.csv')
head(dta)
```

It's also necessary to check the bottom of the dataset to make sure the format consistency and be aware of situations like comment lines at the end of a dataset.

```{r}
tail(dta)
```

The data is formated consistently and ready for analysis. Now let's check overall structure.

```{r}
str(dta)
```

Observations:

* Sample size is 1599, with 13 variables, which all are numerical type.
* There are 11 variables with continuous numerical value, which are physicochemical attributes of wine samples.
* There are 2 variables represented by integers:
    - *X*: representing the index of sample identifier, which will not be included for further analysis.
    - *quality*: according to source description, quality data is developed based on the median value of sensory evaluation given by at least three wine experts. Wine experts judged those samples using the rule that "0 means very bad and 10 means excellent", which means the larger the quality number, the higher the wine quality. This is also the dependent variable of interest and the next step is to look at how the quality distributed in our sample data.

```{r}
# remove 1st column X
dta$X <- NULL

```


## Univariate Plot & Analysis

The focus of this section is to understand the distribution characteristics for each variable, which would help to reveal data central tendency, extreme outliers as well as any needs for data transformation.

### Dependent Variable

We start with creating a bar plot for the dependent variable *quality*. 

```{r}
# bar plot for quality
ggplot(dta, aes(x=factor(quality))) + geom_bar() + xlab("Quality")

```

Quality in our samples ranges from 3 to 8, and the majority samples have quality score 5 or 6. Noticed that the absence of "Excellent:10" or "Very bad:0" scores probably indicates there is no outliers in terms of red wine quality, although it is also possible due to that people tend to avoid giving extreme judgements. 

Let's also see the statistic summary.

```{r}
summary(dta$quality)
```

Observations:

* Quality score 5 and 6 also mark the 1st and 3rd quartile of data respectively. 
    - 25% of samples with quality score below 5
    - 75% of samples with quality score above 6.
* The quality score is created by taking the median of at least three evaluations. This measure can be considered as a method to counter the personal bias in terms of flavor preferences. Therefore, we can assume the quality score is unbiased and score is directly related wine quality.
* However, it might be safe to interpret that wine samples with quality score 8 are much better than those with quality score 3 , it is hard to differenciate whether the differnce in score 4 group and score 3 group is due to quality variation or due to human interpretation on number categories. For example, one expert might use 8 to represent high quality sensory experience with one sample, and another expert might think 7 already represent high quality wine. In other words, though we know the higher the better, the variance may come from human
perception differences on those number categories. 
* Therefore, the next step is to create a new categorical variable "level" to represent wine quality in 3 levels to reduce the perception noise:

  - "Low": quality score 3 or 4
  - "Medium": quality score 5 or 6
  - "High": quality score 7 or 8

```{r}
# Define a new variable: level

dta$level[dta$quality>=7]<- "High"
dta$level[dta$quality>=5 & dta$quality<7]<- "Medium"
dta$level[dta$quality<5]<- "Low"
dta$level<-factor(dta$level,levels=c("Low","Medium","High"))

```

```{r}
# Replot the quality data

ggplot(dta, aes(factor(quality), fill=level)) + geom_bar() + xlab("Quality")

```

The categorical variable *level* will be our new dependent variable. Instead of predicting the exact quality score, our goal is to predict the quality level given certain physicochemcial attributes.

The next step is to check the independent variable group.

### Independent Variables

Independent variable group includes 11 attributes and all of them are continuous numerical variables. To better visualize the central tendency as well as detect the presence of any outliers, it would be helpful to overlay the key statistics like median and outliers on distribution plots. Since there are 11 of them, we will write functions for outlier computation as well as for univariate distribution plotting. 

```{r}

# Function to compute median and outliers given column number dataset

univ_line <- function(feature){
  valmedian <- median(dta[[feature]])
  valiqr <- IQR(dta[[feature]])
  val_q1 <- valmedian - valiqr
  val_q3 <- valmedian + valiqr

# Lower-bound outlier can not be nagative
  val_outlier <- c(max(val_q1-1.5*valiqr,0),val_q3+1.5*valiqr) 
  val_lines <- c(val_outlier, valmedian)
  
  return(val_lines)
}

```

```{r}

# Function for plotting univariate histogram given variable name

univ_hist <- function(feature) {
  ggplot(dta,aes_string(x = feature)) +
    geom_histogram(aes(y=..density..), colour="black", fill="white") + 
    geom_density(alpha=.2, fill="#FF6666") +
    geom_vline(xintercept=univ_line(feature), linetype="longdash",colour="red") 
}

```

```{r}

# 1. fixed.acidity 

univ_hist("fixed.acidity") + 
  annotate("text", univ_line("fixed.acidity")+1, y=0.3, label=c("Lower Outlier","Upper Outlier","Median"))

summary(dta$fixed.acidity)


```

There are 3 red dash lines imposed to histogram-density plot to mark the lower-bound outlier, median and upper-bound outlier respectively.

Fixed acidity falling within a range from 4.6 to 15.9 is close to Gaussian distribution. There are a few samples above upper-bound outlier line but no special handling is required for this attribute. 

```{r}
# 2. volatile.acidity 

univ_hist("volatile.acidity")
summary(dta$volatile.acidity)
```

Volatile acidity seems to have bimodal distribution, as the kernel density plot suggested. This attribute might be a promising predictor since two modes might be signal for different classes in our new dependent variable *level*. There are also a few upper-bound outliers observed, but no special outlier handling is required. 

```{r}
# 3. citric.acid

univ_hist("citric.acid")
summary(dta$citric.acid)
```

Citric acid data have three peaks clearly observed in both histogram and density smooth line. This attribute also seems to be a promising predictor.

For this attribute, data ranges from 0 to 1, sample mean is very close to median, and data has no outliers. 

```{r}
# 4. residual.sugar

univ_hist("residual.sugar")
summary(dta$residual.sugar)

```

Residual sugar has a positively skewd distribution with noticable outliers. 

```{r}
outlier <- max(univ_line("residual.sugar"))
cat("\n")
cat('Upper-bound Outlier is:', outlier)
```


As we can see, the upper-bound outlier is about 4, which is far less than the maximum value 15.5 in sample data. The The presence of outliers might affect both assumptions and results of logistic regression model, since outliers could add huge penalty when residual error is squared. 

There are a couple of ways to handle outliers:

* remove outliers from original dataset: not adopted since we are not sure whether the outliers are part of data characteristics or due to incorrect collection process.
* data transformation: 
    - scale transformation for better view of central tendency
    - actual transformaiton to variables: this step might be needed if we decide to use this variable for prediction model. 

Both Square root and log transformations can pull in large numbers, so let's compare them both:

```{r, fig.height=3.5, fig.width=8.5}
# 4. residual.sugar

# calculate the intercept lines for square transformated data
g1_intercept <- sqrt(univ_line("residual.sugar"))
g1 <- ggplot(dta,aes(sqrt(residual.sugar))) +
    geom_histogram(aes(y=..density..), colour="black", fill="white") +
    geom_density(alpha=.2, fill="#FF6666") +
    geom_vline(xintercept=g1_intercept, linetype="longdash",colour="red") +
  ggtitle("Square Root Transformation")

# calculate the intercept lines for Log10 transformated data
g2_intercept <- log10(univ_line("residual.sugar"))
g2 <- ggplot(dta,aes(log10(residual.sugar))) +
    geom_histogram(aes(y=..density..), colour="black", fill="white") +
    geom_density(alpha=.2, fill="#FF6666") +
    geom_vline(xintercept=g2_intercept, linetype="longdash",colour="red")+
  ggtitle("Log10 Transformation")

grid.arrange(g1,g2, ncol=2)

```

Although outliers can still be observed after both transformation, Log10 transformation provides a better performance--the kernel density plot is close to bell shape, while the distribution produced by square root transformationdistribution still has a relatively obvious tail on the right. So for further analysis, log10 transformation can be applied if residual sugar is selected as predictor.
 
At this point, we can use log scale to better view the original data range:
    
```{r}
# 4. residual.sugar

univ_hist("residual.sugar") +
    scale_x_log10(
   breaks = scales::trans_breaks("log10", function(x) 10^x),
   labels = scales::trans_format("log10", scales::math_format(10^.x))
    ) +
  annotation_logticks(sides="b")

```

From the above log-scaled plot, we can see that data within range 1 to 4 is very close to normal distribution.

```{r}
# 5. chlorides

univ_hist("chlorides")
summary(dta$chlorides)
```

Chlorides distribution is even more positively skewed than residual sugar: the maximum value goes up to 0.6, which is about 60 times larger than the 3rd quartile value 0.09. Similarly, to pull in large numbers, we can apply log10 transformation if this varaible is selected as predictor. For now, we just use log-scaling to better view the distribution.

```{r}
# 5. chlorides

# Apply log-scaling for variable chlorides
univ_hist("chlorides") +
    scale_x_log10(
   breaks = scales::trans_breaks("log10", function(x) 10^x),
   labels = scales::trans_format("log10", scales::math_format(10^.x))
    ) +
  annotation_logticks(sides="b")

# print outlier
outlier <- max(univ_line("chlorides"))
cat("\n")
cat('Upper-bound Outlier is:', outlier)

```

After transformation, we can see the x-axis label 10^(-1) is in the middle of median and upper bound outlier. And it is also easy to see that the majority data falls within 0.04 to 0.129 (upper-bound outlier), with the median value 0.08.

```{r}
# 6. free.sulfur.dioxide

univ_hist("free.sulfur.dioxide")
summary(dta$free.sulfur.dioxide)

```

Free sulfur dioxide is slightly positive skewed, which the maximum value 72 is about 3.5 times of third quartile.For this variable, outlier handling is not needed since the scale can capture the majority of sample data.
    
   
```{r}
# 7. total.sulfur.dioxide

univ_hist("total.sulfur.dioxide")
summary(dta$total.sulfur.dioxide)

```

The distribution shape for total sulfur dioxide is very similar to free sulfur dioxide, which makes sense since free sulfur dioxide is a subset of total sulfur dioxide. However the ratio of maximum value to 3rd quartile (4.7) is greater than the same ratio observed in free.sulfur dioxide, which also makes sense since the subset only contributes part of the variance. Now let's compare their variance:

```{r}
sulf_var <- data.frame(var(dta$free.sulfur.dioxide),
                       var(dta$total.sulfur.dioxide))
sulf_var

```

Not surprised, total sulfur dioxide does have larger variance than free sulfur dioxide. For total sulfur dioxide, there is no need for transformation since the scale can still depict the majority trend.

```{r}
# 8. density

univ_hist("density")
summary(dta$density)

```

Density looks highly like a normal distribution, with data evenly distributed two sides from median. It is worth noticed that the data range for this variable is very small--from 0.99 to 1. The majority samples with density less than 1 makes sense since the density of ethanol is less than water. One possible cause for wine denser than water could be high sugar level since the density of sucrose is about 1.59 g/cm^3^. We can further compare relationship between density and residual sugar in bivariate section.

```{r}
# 9. pH

univ_hist("pH")
summary(dta$pH)

```

All wine samples with pH values lie on the acidic side of pH spectrum: from 2.7 to 4. The pH distribution is another one very close to Gaussian/normal bell shape.

```{r}
# 10. sulphate

univ_hist("sulphates")
summary(dta$sulphates)
```

For sulphates, we can see a light tail on the positive side. Data ranges from 0.33 to 2, and the ratio of maximum value to 3rd quartile is about 3, also indicating the slight positive skewed trend, but no need for outlier handling.

```{r}
# 11. alcohol

univ_hist("alcohol")
summary(dta$alcohol)
```

For alcohol, the majority of data within range of 8.8 to 12.5. We can observe the two peaks revealed by the above plot, which might be signal for another promising predictor.

Observations for univariate exploratory:

* 2 attributes are normallly distributed: density and pH.
    - density : there are samples with density greater than 1. In bivariate section, we will further investigate the cause by examing how density is related to residual sugar. 
* the other 9 attributes are more or less positively skewed:
    - residual.sugar and chlorides are highly skewed to the right, log-scalling is applied to both variables to better view the central tendency.
    - Volatile acidity, citric.acidity and alcohol seem to be promissing predictor since there are more than one peaks observed on histogram or kernel density plots. This kind of characteristic might be signal for different categories of new response variable *level*.

## Bivariate Plot & Analysis

One comment from the meta data, variables in this dataset might be correlated. This indicates us two things:

* If correlation relationship observed between independent variable and dependent variable, the strenth of correlation couldl us to select predictors;
* However, if strong correlation observed between independent variable pairs, we must carefully remove some variables to reduce the multicolinearity issue.

Since correlation is a measure for linear relationship, which is sensitive to outliers, we will apply log10 transformation to residual sugar and chlorides to reduce the effect.

Another note is that  *quality* instead of *level* will be used to compute correlation matrix for the below reasons:

* Though we can use ployserial correlation to quantify the relathionship between continous variable and dichotomous variable, this method would mute most of subtle differences since only means were compared for three categories.
* level is a variable derived from quality, and intunitively we are interested in whether increasing a certain attribute would contribute to increasing or decreasing in wine quality response. Therefore, if correlation relationship is observed between an independent variable and original dependent variable *quality*, we can extend the conclusion to *level* too. 
* For simpliticity and consistency, correlation matrix for all numerical type is relative easy to read and to interpret.

Therefore, we will first re-organize the dataset, and then check the correlation. To better view the strength of association patterns, we will customize a correlation heatmap:

```{r}
# Create dataset not including "level"
dt <- dta[,1:12]
dt$log_residual.sugar <- log10(dt$residual.sugar)
dt$log_chlorides <- log10(dt$chlorides)
dt$residual.sugar <- NULL
dt$chlorides <- NULL

# calculate correlation
cormat <- round(cor(dt),2)
```

```{r}
# ----Function for Triangle style -------------

# Get lower triangle of the correlation matrix
  get_lower_tri<-function(cormat){
    cormat[upper.tri(cormat)] <- NA
    return(cormat)
  }
# Get upper triangle of the correlation matrix
  get_upper_tri <- function(cormat){
    cormat[lower.tri(cormat)]<- NA
    return(cormat)
  }

```

```{r}
# Apply Upper triangle style
upper_tri <- get_upper_tri(cormat)

# Melt the correlation matrix
melted_cormat <- melt(upper_tri, na.rm = TRUE)

# Create a ggheatmap
ggheatmap <- ggplot(melted_cormat, aes(Var2, Var1, fill = value))+
 geom_tile(color = "white")+
 scale_fill_gradient2(low = "blue", high = "red", mid = "white", 
   midpoint = 0, limit = c(-1,1), space = "Lab", 
    name="Pearson\nCorrelation") +
  theme_minimal()+ # minimal theme
 theme(axis.text.x = element_text(angle = 45, vjust = 1, 
    size = 12, hjust = 1))+
 coord_fixed()

```

```{r, fig.height=6.5, fig.width=6.5}
ggheatmap + 
geom_text(aes(Var2, Var1, label = value), color = "black", size = 4) +
theme(
  axis.title.x = element_blank(),
  axis.title.y = element_blank(),
  panel.grid.major = element_blank(),
  panel.border = element_blank(),
  panel.background = element_blank(),
  axis.ticks = element_blank(),
  legend.justification = c(1, 0),
  legend.position = c(0.6, 0.7),
  legend.direction = "horizontal")+
  guides(fill = guide_colorbar(barwidth = 7, barheight = 1,
                title.position = "top", title.hjust = 0.5))
```

According to Evans (1996), correlation value r can be interpreted as below:

*  .00-.19 \ "very weak"
*  .20-.39 \ "weak"
*  .40-.59 \ "moderate"
*  .60-.79 \ "strong"
*  .80-1.0 \ "very strong"

In relate to our heat map, we can summarize the association patterns in two groups:

1. Strong Correlation:
    - fixed.acidity vs citric.acid: positive +
    - fixed.acidity vs density: positive +
    - free.sulfur.dioxide vs total.sulfur.dioxide: positive +
    - fixed.acidity vs pH: negative -

2. Moderate Correlation:
    - alcoholvs quality: positive +
    - density vs log10 transformed residual.sugar: positive +
    - volatile.acidity vs citric.acid: negative -
    - citric.acid vs pH: nagative -

Among the above 8 pairs, we can see only alcohol show moderate correlation to quality.

Fixed.acidity is strongly correlated to citric.acid, density and pH. Free.sulfur.dioxide is strongly correlated to total.sulfur.dioxide, which again is not surprising since it is a subset. So for any strong correlated pairs, we need pick candidate predictors very carefully to minimize colinearity noise. 

Another interesting observation is that after applying transformation to residual sugar, this variable do observe moderate correlation to density. In the univariate section, we did suspect that the cause for some wine samples denser than water could be high sulcose content. Moderate correlation seems encouraging our suspicion.

To end this correlation check, We can add one more comparison by computing the polyserial correlation between *alcohol* vesus categorical variable *level*:

```{r}
## Correlation: Categorical Variable
cor1 <- cor(dta$alcohol,dta$quality)
cor2 <- polyserial(dta$alcohol,dta$level)
cor3 <- polyserial(dta$quality, dta$level)

cat('Correlation Alcohol vs Quality ', cor1)
cat("\n")
cat('Correlation Alcohol vs Level:', cor2)
cat("\n")
cat('Correlation Quality vs Level:', cor3)
```

Now we can assured that the association patterns analyzed for *quality* are reprentative for our categorical variable *level* as well.

The next step is to visualize the pairs we found having strong or moderate correlation.
Let's begin with the strong correlated pairs:

```{r, fig.height=6, fig.width=6}
g1 <- ggplot(dt, aes(x=citric.acid, y=fixed.acidity)) + 
  geom_jitter(alpha=0.1) + geom_smooth(method="lm", se=FALSE)

g2 <- ggplot(dt, aes(x=fixed.acidity, y=density)) + 
  geom_jitter(alpha=0.1) + geom_smooth(method="lm", se=FALSE)

g3 <- ggplot(dt, aes(x=fixed.acidity, y=pH)) + 
  geom_jitter(alpha=0.1) + geom_smooth(method="lm", se=FALSE)

g4 <- ggplot(dt, aes(x=free.sulfur.dioxide, y=total.sulfur.dioxide)) + 
  geom_jitter(alpha=0.1) + geom_smooth(method="lm", se=FALSE)

grid.arrange(g1,g2,g3,g4, ncol=2)

```

Citric.acid is one of the contributer to fixed.acidity. So it's likely that the increase of citric.acid caused the increase of fixed.acidity, not the other way around.

Similarly, the increase of free.sulfur.dioxide can explain the increasing trend in total.sulfur.dioxide, not the other way around.

The positive relationship between fixied.acidity and density might just revealed another factor contributing to denser wine.

The causal direction is clear between fixed.acidity and pH: the higher the fixed.acidity, the lower pH value.

Therefore, for further reference, if we need to choose predictors among those pair, we should prioritize the ones with causal power instead of the responsive ones.

Next, let's also visualize the pairs with moderate correlate correlation:


```{r, fig.height=4, fig.width=6}
g1 <- ggplot(dt, aes(x=citric.acid, y=volatile.acidity)) + 
  geom_jitter(alpha=0.1) + geom_smooth(method="lm", se=FALSE)

g2 <- ggplot(dt, aes(x=citric.acid, y=pH)) + 
  geom_jitter(alpha=0.1) + geom_smooth(method="lm", se=FALSE)

grid.arrange(g1,g2, ncol=2)

```

Actually, citric.acid is non-volatile organic acids, so there should not be any causal effect between citric acid and volatile.acidity. Instead, it is due to the ratio competiton between volatile.acidity and non-volatile acidity.

It is clear that the higher ciric.acid would contribute to lower pH value.

Next, let's review the causes for denser wine.
Wine samples with density greater than 1 have been defined as *Dense Wine*, and those with density less or equal than 1 have been defined as *Regular Wine*.

```{r, fig.height=6.5, fig.width=5}
g1 <- ggplot(dt, aes(x=log_residual.sugar, y=density, 
                color=ifelse(density>1, "Dense Wine","Regular Wine"))) + 
  labs(color="Density") +
  geom_point(alpha=0.4) + 
  geom_smooth(method=lm, se=FALSE)+
  theme(legend.position="top")
  
g2 <- ggplot(dt, aes(x=fixed.acidity, y=density, 
                color=ifelse(density>1, "Dense Wine","Regular Wine"))) +
  labs(color="Density") +
  geom_point(alpha=0.4) + 
  geom_smooth(method=lm, se=FALSE)+
  theme(legend.position="bottom")

grid.arrange(g1,g2)

```

For Regular Wine group, the residual sugar level is condensed in the range from 10^0.2^ (1.58) to 10^0.4^ (2.51). While for the Dense Wine group, the majority data are within range from 2 to 4. 

Since we also suspect that fixed.acidity might be another factor contributing wine density, let's create similar plots to see whether that will help explain the variance.


Though the overlap between two ranges might suggest there are other parameters also contributing wine density, we can still infer that density is positive correlated to residual sugar for both dense wine and regular wine groups. For future study topics, we can run t-test to see whether the difference is statistically significant, which is not included in this study. 

In order to build predictive model, we need to understand the association patterns between these physicochemical attributes  and our dependent variable *level*.

Since residual sugar and chlorides have significant outliers, we will apply log transformation for these two variables first, then develop a function to create boxplots for all 11 attributes.

```{r}
# Apply log10 transformation to residual sugar and chlorides.
trans <- dta[,2:12] # No need to include column X and quality
trans$log_residual.sugar <- log10(trans$residual.sugar)
trans$log_chlorides <- log10(trans$chlorides)

trans<- subset(trans, select=-c(residual.sugar,chlorides))
trans$level <- dta$level

```

```{r}
# Create a empty list to hold plots
plot_list <- list()
nplot = 11

ind_names <- names(trans)[1:11]

for (i in seq(nplot)){
  new_plot <- ggplot(trans,aes_string(y=ind_names[i])) +
    geom_boxplot(aes(x=level))
  plot_list <- c(plot_list,list(new_plot))
}

```

Boxplot for acidity group:

```{r, message=FALSE, warning=FALSE}
# print the first 3 boxplots on plot_list
do.call(grid.arrange, c(plot_list[1:3], ncol=3))
```

Observations: 
* positive correlated to level: fixed.acidity and citric.acid
* negative correlated to level: volatile.acidity

Boxplot for sulfur dioxide group:

```{r, message=FALSE, warning=FALSE}
# print the 4th and 5th boxplots on plot_list
do.call(grid.arrange, c(plot_list[4:5], ncol=2))
```

It seems that there's no clear trend between sulfur dioxide group with wine quality level.

```{r}
# print the 6th to  9th boxplots on plot_list
do.call(grid.arrange, c(plot_list[6:9], ncol=2))

```

Observations: 
* positive correlated to level: sulphates and alcohol
* negative correlated to level: pH
* unclear trend: density

Next, let's look at the log-transformed data group: residual sugar and chlorides.

```{r}
# print the 10th and 11th boxplots on plot_list
do.call(grid.arrange, c(plot_list[10:11], ncol=2))

```

Observations: 
* positive correlated to level: log-transformed residual.sugar
* negative correlated to level: log-transformed chlorides

Now let's regroup independent variables in three categories based on their relationship with dependent variable level:

* Increasing trend group: 
   - fixed.acidity
   - citric.acid
   - sulphates
   - alcohol
   - log_residual.sugar
* Decreasing trend group: indicating negative correlation between physicochemcial attrbute and wine quality level. 
   - volatile.acidity
   - pH
   - log_chlorides
* Unclear trend group: no specific correlation pattern observed between physicochemcial attrbute and wine quality level. 
   _ free.sulfur.dioxide
   - total.sulfur.dioxide
   - density
   



```{r}
Incre_group <- subset(trans, select = c(fixed.acidity, citric.acid, sulphates, alcohol, log_residual.sugar, level))
Decre_group <- subset(trans, select = c(volatile.acidity, pH, log_chlorides, level))
Unclear_group <- subset(trans, select = c(free.sulfur.dioxide, total.sulfur.dioxide, density, level))



```

```{r}
# Create a empty list to hold plots

Incre_list <- list()
nplot = 5

ind_names <- names(Incre_group)[1:5]

for (i in seq(nplot)){
  new_plot <- ggplot(Incre_group,aes_string(y=ind_names[i])) +
    geom_boxplot(aes(x=level, fill=level)) +
    scale_fill_brewer(palette = "RdPu", guide=FALSE) +
    xlab("")
  
  Incre_list <- c(Incre_list,list(new_plot))
}


# Print plots
do.call(grid.arrange, c(Incre_list, list(ncol=3,
        main=textGrob("Positive Correlated to Wine Quality Level", 
                      gp=gpar(fontsize=12,font=8), just="top"))))

```

The above 5 physicochemcial attrbutes shown positive correlation to wine quality level can be considered as candidate predictors.

```{r}
# Create a empty list to hold plots

Decre_list <- list()
nplot = 3

ind_names <- names(Decre_group)[1:3]

for (i in seq(nplot)){
  new_plot <- ggplot(Decre_group,aes_string(y=ind_names[i])) +
    geom_boxplot(aes(x=level, fill=level)) +
    scale_fill_brewer(guide=FALSE) +
    xlab("")
  
  Decre_list <- c(Decre_list,list(new_plot))
}


# Print plots
do.call(grid.arrange, c(Decre_list, list(ncol=3,
        main=textGrob("Negative Correlated to Wine Quality Level", 
                      gp=gpar(fontsize=12,font=8), just="top"))))
```

The above 3 physicochemcial attrbutes shown negative correlation to wine quality level can also be considered as candidate predictors.

```{r}
# Create a empty list to hold plots

Unclear_list <- list()
nplot = 3

ind_names <- names(Unclear_group)[1:3]

for (i in seq(nplot)){
  new_plot <- ggplot(Unclear_group,aes_string(y=ind_names[i])) +
    geom_boxplot(aes(x=level, fill=level)) +
    scale_fill_brewer(palette = "Purples", guide=FALSE) +
    xlab("")
  
  Unclear_list <- c(Unclear_list,list(new_plot))
}


# Print plots
do.call(grid.arrange, c(Unclear_list, list(ncol=3,
        main=textGrob("Unclear Trend Group", 
                      gp=gpar(fontsize=12,font=8), just="top"))))
```

The above 3 physicochemcial attrbutes will not be included for further analysis since there's no clear correlation to wine quality level.

So far, we shrinked down predictor variables from 11 attributes to 8. 

## Multi-variate Plot & Analysis


```{r}
ggplot(dta, aes(y=alcohol, x=log10(residual.sugar), colour=level)) +
  geom_point(alpha=0.3, position = position_jitter()) +
  stat_smooth(method="lm", se=FALSE)
```



```{r}
ggplot(dta, aes(x=density, y= alcohol, colour=level)) +
  facet_wrap(~ level) +
  geom_point(alpha=0.3, position="jitter") +
  geom_boxplot(alpha=0, colour="black")
```


```{r}


ggplot(dta, aes(x=alcohol, y=density)) +
  geom_smooth(aes(colour=level, fill=level)) + facet_wrap(~level)
```


```{r}
ggplot(dta, aes(alcohol, fill=level, colour=level))+
  geom_density(alpha=0.1)
```







The next step is to build a prediction model by fitting the above 7 predictors.

# Prediction Model

Since our dependent variable is categorical data, we need to select muli-class logistic regression model.

The dataset for prediction model should include 7 predictors and 1 target variable.

## Data Partition: Training & Test 

We first need to split dataset into training and test subsets. Training data for prediction model training, and test data for model performance evaluation. Random sampling will applied within dependent variable wine quality level for total 1599 samples, which will result in 70% training and 30% test data.

```{r}

candi$level <- dta$level

intrain<-createDataPartition(y=candi$level,p=0.7,list=FALSE, times=1)
candi_train<-candi[intrain,]
candi_test<-candi[-intrain,]

p1<-ggplot(candi, aes(level, fill=level)) + geom_bar() + labs(title="Original Dateset")+ coord_cartesian(ylim=c(0,1599)) + theme(axis.title.x=element_blank(), axis.text.x=element_blank(),axis.ticks.x=element_blank())

p2<-ggplot(candi_train, aes(level, fill=level)) + geom_bar() + labs(title="70% as Training") + coord_cartesian(ylim=c(0,1599)) + theme(axis.title.x=element_blank(), axis.text.x=element_blank(),axis.ticks.x=element_blank())

p3<-ggplot(candi_test, aes(level, fill=level)) + geom_bar()  + labs(title="30% as Test") + coord_cartesian(ylim=c(0,1599)) + theme(axis.title.x=element_blank(), axis.text.x=element_blank(),axis.ticks.x=element_blank())

source('~/Documents/Udacity/P4/grid_arrange_shared_legend.r')

grid_arrange_shared_legend(p1,p2,p3, ncol=3)
```

## Random Forest Prediction Model

For this study, Random Forest algorithm is selected due to its advantage on both accuracy and efficiency.

Build prediction model using randomForest on the training set.

```{r}
wine_rf <- randomForest(level ~ citric.acid + log_chlorides + sulphates +
                             volatile.acidity + pH + log_residual.sugar + alcohol,
                           data=candi_train)
wine_rf
```
The overall error rate estimated on training dataset is below 14%.

## Model Performance

Now let's check the prediction performance on test dataset.

```{r echo=FALSE, message=FALSE, warning=FALSE}
wine_pred <- predict(wine_rf, candi_test)
compare<-table(Observed=candi_test$level, Predicted=wine_pred)
compare

df<-as.data.frame(compare)
correct <-subset(df,df$Observed==df$Predicted)
accuracy <- (sum(correct$Freq))/nrow(candi_test)
cat("\n")
cat('Accuracy of Prediction Model is:', accuracy)
```
The prediction model achieved 87% accuracy rate.

In the univariate plot section, we proposed that volatile.acidity, alcohol and citric.acid would be potential predictors based on observed two or three peaks on historgram.

In the bivariate plot section, boxplots also revealed that citric.acid, sulphates, log_residual.sugar and alcohol all show positive correlation with wine quality level.

Now let's visualize the rank of all 7 attributes based on the variable importance.

```{r echo=FALSE, message=FALSE, warning=FALSE}
# Get importance
importance <- importance(wine_rf)
varImportance <- data.frame(Variables = row.names(importance), 
                      Importance = round(importance[ ,'MeanDecreaseGini'],2))

# Create a rank variable based on importance
rankImportance <- varImportance %>%
  mutate(Rank = paste0('#',dense_rank(desc(Importance))))

# Use ggplot2 to visualize the relative importance of variables
ggplot(rankImportance, aes(x = reorder(Variables, Importance), 
    y = Importance)) + geom_bar(stat="identity") + 
  labs(x = 'Variables') +
  coord_flip() 
  
```

Our previous assumptions on alcohol, volatile.acidity, citric.acid are ranked as No.1, No.2, No.5 respectively.
The most important variable alcohol is positively correlated to wine quality level, and the second important variable volatile.acidity is negatively correlated to wine quality level.


# Summary

By apply random forest algorithm, we achieved 87% accuracy on predicting red wine quality level by 7 physicochemico attributes.

The process that helped us to narrow down the 7 predictors from 11 variable is mainly by EDA phase. The below section includes three final plots developed during EDA phase. 

## Final Plots

Final Plot 1: The reason to keep this plot is because we can see 3 peaks from histogram and 2 mode from kerney density. Together it suggested volatile.acidity might be a good predictor for wine quality level, and this has been further confirmed as we can see it is the 2nd most important variable in the Random Forest model.  Another reason is that the statistic outliers are overlaied in a compact and informative way. 

```{r}
# get column index for volatile.acidity
var_col <- grep("volatile.acidity", colnames(dta))

# calculate the intercept lines for square transformated data
g1_intercept <- col_line(var_col)

ggplot(dta,aes(volatile.acidity)) +
    geom_histogram(aes(y=..density..), colour="black", fill="white") +
    geom_density(alpha=.2, fill="#FF6666") +
    geom_vline(xintercept=g1_intercept, linetype="longdash",colour="red")



```

Final Plot 2: this plot is selected because it reveals the positive correlation between residual sugar and density. 
This plot also compared Regular Wine group vesus Dense Wine group, and the trend holds valid for both, though a overlaped range is observed. For future study topics, t-test can be selected to see whether the difference is statistically significant.

```{r}
ggplot(dta, aes(x=residual.sugar, y=density, 
                color=ifelse(density>1, "Dense Wine","Regular Wine"))) + 
  labs(color="Density") +
  geom_point(alpha=0.4) + 
  geom_smooth(method=lm, se=FALSE) +
  scale_x_log10(
   breaks = scales::trans_breaks("log10", function(x) 10^x),
   labels = scales::trans_format("log10", scales::math_format(10^.x))
    ) +
  annotation_logticks(sides="b")
  

```

Final Plot 3: this plot is selected because the below five attributes show clear positive correlation to wine quality level. Alcohol and sulphate are ranked as No.1 and No.3 in terms of their importance for prediction model.

```{r}
# Create a empty list to hold plots

Incre_list <- list()
nplot = 5

ind_names <- names(Incre_group)[1:5]

for (i in seq(nplot)){
  new_plot <- ggplot(Incre_group,aes_string(y=ind_names[i])) +
    geom_boxplot(aes(x=level, fill=level)) +
    scale_fill_brewer(palette = "RdPu", guide=FALSE) +
    xlab("")
  
  Incre_list <- c(Incre_list,list(new_plot))
}


# Print plots
do.call(grid.arrange, c(Incre_list, list(ncol=3,
        main=textGrob("Positive Correlated to Wine Quality Level", 
                      gp=gpar(fontsize=12,font=8), just="top"))))
```

## Reflection

In this study, we applied random forest algorithm to predict wine quality level. The methodology involves two major phases: EDA phase and prediction model development phase.

* EDA phase: it is a highly iterative process and this is also the part I spent most time with. One lesson's learned is that in EDA phase, it is very tempting to going on and on, but it is also very important to know when to stop because real projects will have time and monetary budget. Fortunately at last this EDA phase reached a relative satisfactory status.

* Prediction Model Development: there are a few limitations in this phase.
   - fixed.acidity is eliminated as a predictor because of its colinearity. To complete the model perforamce and feature selection, we should also train a model by including fixed.acidity but removing the other two attributes associated with fixed.acidity. This way we can have a relative comprehensive model representation for potential candidates.
   - Idealy, we should divide dateset into training, validation and test subsets. By doing so, we can train a couple of prediction models by fitting training data, and then compare error rate in validation dataset, at last apply the model with best performance in validation dataset to the test dataset. In this study, the validation process is not included. 



# Reference

1. P. Cortez, A. Cerdeira, F. Almeida, T. Matos and J. Reis. Modeling wine preferences by data mining from physicochemical properties. In Decision Support Systems, Elsevier, 47(4):547-553, 2009.

2. Correlation matrix: An R function to do all you need (http://www.sthda.com/english/wiki/correlation-matrix-an-r-function-to-do-all-you-need#at_pco=smlre-1.0&at_si=589272c605773292&at_ab=per-2&at_pos=3&at_tot=4)

3. Source code for Variable Importance Plot (https://www.kaggle.com/mrisdal/titanic/exploring-survival-on-the-titanic)

4. Evans, J. D. (1996). Straightforward statistics for the behavioral sciences. Pacific Grove, CA: Brooks/Cole
Publishing.

5. Correlation Coefficients: Describing Relationships
(https://www.rasch.org/rmt/rmt193c.htm)

6. ggplot2: Quick correlation matrix heatmap
(http://www.sthda.com/english/wiki/ggplot2-quick-correlation-matrix-heatmap-r-software-and-data-visualization)

